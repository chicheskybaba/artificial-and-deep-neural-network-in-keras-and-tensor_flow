{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "560ebc68-d9ad-4020-8d46-b2877b748710",
   "metadata": {},
   "source": [
    "# CNN 6 - Do Larger Model Lead to Better Performance?\n",
    "- Dataset:\n",
    "    - https://www.kaggle.com/shaunthesheep/microsoft-catsvsdogs-dataset\n",
    "\n",
    "\n",
    "**What you should know by now:**\n",
    "- How to preprocess image data\n",
    "- How to load image data from a directory\n",
    "- What's a convolution, pooling, and a fully-connected layer\n",
    "- Categorical vs. binary classification\n",
    "\n",
    "<br>\n",
    "\n",
    "- First things first, let's import the libraries\n",
    "- The models we'll declare today will have more layers than the ones before\n",
    "    - We'll implement individual classes from TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1148f14-6117-4ab2-8d46-96807f6c8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf5a38-a5d4-46f3-bf99-76028a0a0581",
   "metadata": {},
   "source": [
    "- I'm using Nvidia RTX 3060 TI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699c796a-945c-41f8-9c91-8f051243bdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09eb01-fa4b-44df-b666-e07b9e5eea9f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Load in the data\n",
    "- Use `ImageDataGenerator` to convert image matrices to 0-1 range\n",
    "- Load in the images from directories and convert them to 224x224x3\n",
    "- For memory concerns, we'll lower the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fd6610-ba6d-4ffb-8add-cb7e3120eb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20030 images belonging to 2 classes.\n",
      "Found 2478 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "valid_datagen = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    directory='data/train/',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "valid_data = valid_datagen.flow_from_directory(\n",
    "    directory='data/validation/',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f854f-bb13-4927-80ee-61f6e9117bd2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Model 1\n",
    "- Block 1: Conv, Conv, Pool\n",
    "- Block 2: Conv, Conv, Pool\n",
    "- Block 3: Flatten, Dense\n",
    "- Output\n",
    "\n",
    "<br>\n",
    "\n",
    "- We won't mess with the hyperparameters today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e293a9c-7fcf-4cbc-b5fd-55a81e1394f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "626/626 [==============================] - 40s 61ms/step - loss: 0.6586 - accuracy: 0.6149 - val_loss: 0.6115 - val_accuracy: 0.6804\n",
      "Epoch 2/10\n",
      "626/626 [==============================] - 37s 59ms/step - loss: 0.5223 - accuracy: 0.7422 - val_loss: 0.5265 - val_accuracy: 0.7554\n",
      "Epoch 3/10\n",
      "626/626 [==============================] - 38s 60ms/step - loss: 0.4073 - accuracy: 0.8125 - val_loss: 0.5061 - val_accuracy: 0.7571\n",
      "Epoch 4/10\n",
      "626/626 [==============================] - 38s 61ms/step - loss: 0.2476 - accuracy: 0.8942 - val_loss: 0.6336 - val_accuracy: 0.7672\n",
      "Epoch 5/10\n",
      "626/626 [==============================] - 38s 61ms/step - loss: 0.1004 - accuracy: 0.9625 - val_loss: 1.0141 - val_accuracy: 0.7571\n",
      "Epoch 6/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.0419 - accuracy: 0.9863 - val_loss: 1.3990 - val_accuracy: 0.7700\n",
      "Epoch 7/10\n",
      "626/626 [==============================] - 38s 61ms/step - loss: 0.0352 - accuracy: 0.9894 - val_loss: 1.2963 - val_accuracy: 0.7680\n",
      "Epoch 8/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.0263 - accuracy: 0.9932 - val_loss: 1.4017 - val_accuracy: 0.7684\n",
      "Epoch 9/10\n",
      "626/626 [==============================] - 38s 61ms/step - loss: 0.0263 - accuracy: 0.9940 - val_loss: 1.3149 - val_accuracy: 0.7780\n",
      "Epoch 10/10\n",
      "626/626 [==============================] - 38s 61ms/step - loss: 0.0237 - accuracy: 0.9940 - val_loss: 1.6602 - val_accuracy: 0.7482\n"
     ]
    }
   ],
   "source": [
    "model_1 = tf.keras.Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), input_shape=(224, 224, 3), activation='relu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model_1.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer=Adam(),\n",
    "    metrics=[BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "model_1_history = model_1.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f516c509-7d70-4fdc-9661-f4a180b1a64d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- Not bad, but we got 75% accuracy on the validation set in notebook 010\n",
    "- Will adding complexity to the model increase the accuracy?\n",
    "\n",
    "## Model 2\n",
    "- Block 1: Conv, Conv, Pool\n",
    "- Block 2: Conv, Conv, Pool\n",
    "- Block 3: Conv, Conv, Pool\n",
    "- Block 4: Flatten, Dense\n",
    "- Ouput\n",
    "\n",
    "<br>\n",
    "\n",
    "- This artchitecture is a bit of an overkill for our dataset\n",
    "- The model isn't learning at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb9fb77-1ead-426f-bf9f-2a959763f7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.7040 - accuracy: 0.4955 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.6932 - accuracy: 0.4959 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.6932 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.6932 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.6932 - accuracy: 0.5006 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "626/626 [==============================] - 40s 64ms/step - loss: 0.6932 - accuracy: 0.4924 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "626/626 [==============================] - 40s 64ms/step - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "626/626 [==============================] - 40s 63ms/step - loss: 0.6932 - accuracy: 0.5023 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "626/626 [==============================] - 40s 64ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "626/626 [==============================] - 40s 64ms/step - loss: 0.6932 - accuracy: 0.5034 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), input_shape=(224, 224, 3), activation='relu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model_2.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer=Adam(),\n",
    "    metrics=[BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "model_2_history = model_2.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dea366-50b9-4310-94ea-f23a9d3a551d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- When that happens, you can try experimenting with the learning rate and other parameters\n",
    "- Let's dial it down a bit next\n",
    "\n",
    "<br>\n",
    "\n",
    "## Model 3 \n",
    "- Block 1: Conv, Conv, Pool\n",
    "- Block 2: Conv, Conv, Pool\n",
    "- Block 3: Flatten, Dense, Dropout, Dense\n",
    "- Output\n",
    "\n",
    "<br>\n",
    "\n",
    "- The first model was better than the second\n",
    "- We can try adding a dropout layer as a regulizer and tweaking the fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3710c286-833c-40e5-8522-57411c146e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.7498 - accuracy: 0.5622 - val_loss: 0.6580 - val_accuracy: 0.6295\n",
      "Epoch 2/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.6101 - accuracy: 0.6744 - val_loss: 0.5645 - val_accuracy: 0.7159\n",
      "Epoch 3/10\n",
      "626/626 [==============================] - 39s 62ms/step - loss: 0.5007 - accuracy: 0.7562 - val_loss: 0.5734 - val_accuracy: 0.7070\n",
      "Epoch 4/10\n",
      "626/626 [==============================] - 39s 63ms/step - loss: 0.3297 - accuracy: 0.8585 - val_loss: 0.7222 - val_accuracy: 0.7038\n",
      "Epoch 5/10\n",
      "626/626 [==============================] - 40s 64ms/step - loss: 0.1246 - accuracy: 0.9556 - val_loss: 1.1581 - val_accuracy: 0.6965\n",
      "Epoch 6/10\n",
      "626/626 [==============================] - 39s 63ms/step - loss: 0.0786 - accuracy: 0.9786 - val_loss: 0.8357 - val_accuracy: 0.6832\n",
      "Epoch 7/10\n",
      "626/626 [==============================] - 40s 64ms/step - loss: 0.0425 - accuracy: 0.9877 - val_loss: 1.3557 - val_accuracy: 0.7006\n",
      "Epoch 8/10\n",
      "626/626 [==============================] - 40s 64ms/step - loss: 0.0277 - accuracy: 0.9934 - val_loss: 2.0383 - val_accuracy: 0.6780\n",
      "Epoch 9/10\n",
      "626/626 [==============================] - 40s 64ms/step - loss: 0.0334 - accuracy: 0.9926 - val_loss: 1.0312 - val_accuracy: 0.6913\n",
      "Epoch 10/10\n",
      "626/626 [==============================] - 40s 64ms/step - loss: 0.0298 - accuracy: 0.9925 - val_loss: 1.5798 - val_accuracy: 0.6985\n"
     ]
    }
   ],
   "source": [
    "model_3 = tf.keras.Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), input_shape=(224, 224, 3), activation='relu'),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(units=512, activation='relu'),\n",
    "    Dropout(rate=0.3),\n",
    "    Dense(units=128),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_3.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer=Adam(),\n",
    "    metrics=[BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "\n",
    "model_3_history = model_3.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f336697-5e77-47f1-a8b2-a572a720ec47",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- It made the model worse\n",
    "- More complex model don't necessarily lead to an increase in performance\n",
    "\n",
    "<br>\n",
    "\n",
    "## Conclusion\n",
    "- There you have it - we've been focusing on the wrong thing from the start\n",
    "- Our model architecture in the notebook 010 was solid\n",
    "    - Adding more layers and complexity decreases the predictive power\n",
    "- We should shift our focus to improving the dataset quality\n",
    "- The following notebook will teach you all about **data augmentation**, and you'll see how it increases the power of our model\n",
    "- After that you'll take your models to new heights with **transfer learning**, and you'll see why coming up with custom architectures is a waste of time in most cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
